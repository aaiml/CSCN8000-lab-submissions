{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part A:\n",
    "\n",
    "Problem Statement: Consider the dataset Credit Card Fraud Detection from Kaggle and build a machine-learning model that detects whether a credit card transaction is fraudulent. Demonstrate the steps of data preprocessing and analysis, consider applying train (0.7) and test (0.3), using the logistic regression to build the model, and evaluate to determine the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are various processes involved in creating a credit card fraud detection model, including data pretreatment, analysis, model creation, and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preprocessing and analysis in Step 1\n",
    "\n",
    "Add the required libraries, then load the data.\n",
    "\n",
    "Explore the dataset to learn about its features, statistics, and organisational structure.\n",
    "\n",
    "Check for missing values and take the proper action (such as impute or remove) if any are found.\n",
    "\n",
    "If necessary, carry out feature engineering.\n",
    "\n",
    "Since credit card fraud typically occurs seldom relative to regular transactions, the dataset should be balanced.\n",
    "\n",
    "70% of the data should be used for training and 30% should be used for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time      0\n",
      "V1        0\n",
      "V2        0\n",
      "V3        0\n",
      "V4        0\n",
      "V5        0\n",
      "V6        0\n",
      "V7        0\n",
      "V8        0\n",
      "V9        0\n",
      "V10       0\n",
      "V11       0\n",
      "V12       0\n",
      "V13       0\n",
      "V14       0\n",
      "V15       0\n",
      "V16       0\n",
      "V17       0\n",
      "V18       0\n",
      "V19       0\n",
      "V20       0\n",
      "V21       0\n",
      "V22       0\n",
      "V23       0\n",
      "V24       0\n",
      "V25       0\n",
      "V26       0\n",
      "V27       0\n",
      "V28       0\n",
      "Amount    0\n",
      "Class     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(r\"C:\\Users\\asus\\Downloads\\archive (1).zip\")\n",
    "\n",
    "# Check for missing values\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# Splitting the data into features (X) and target variable (y)\n",
    "X = data.drop(\"Class\", axis=1)\n",
    "y = data[\"Class\"]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Perform feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Model Building (using polynomial Kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;poly&#x27;, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;poly&#x27;, random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(kernel='poly', random_state=42)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the necessary libraries\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Create an SVM model with Polynomial kernel\n",
    "model_poly = SVC(kernel='poly', random_state=42)\n",
    "\n",
    "# Train the model using the training data\n",
    "model_poly.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (Polynomial Kernel): 0.9994850368081645\n",
      "Classification Report (Polynomial Kernel):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     85307\n",
      "           1       0.90      0.76      0.83       136\n",
      "\n",
      "    accuracy                           1.00     85443\n",
      "   macro avg       0.95      0.88      0.91     85443\n",
      "weighted avg       1.00      1.00      1.00     85443\n",
      "\n",
      "Confusion Matrix (Polynomial Kernel):\n",
      "[[85295    12]\n",
      " [   32   104]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Make predictions on the test data for the polynomial kernel\n",
    "y_pred_poly = model_poly.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance for the polynomial kernel\n",
    "accuracy_poly = accuracy_score(y_test, y_pred_poly)\n",
    "print(\"Accuracy (Polynomial Kernel):\", accuracy_poly)\n",
    "\n",
    "# Print classification report and confusion matrix for the polynomial kernel\n",
    "print(\"Classification Report (Polynomial Kernel):\")\n",
    "print(classification_report(y_test, y_pred_poly))\n",
    "\n",
    "print(\"Confusion Matrix (Polynomial Kernel):\")\n",
    "print(confusion_matrix(y_test, y_pred_poly))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code using sigmoid kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "Accuracy (SVM with Sigmoid Kernel - Tuned): 0.9986891846026006\n",
      "Classification Report (SVM with Sigmoid Kernel - Tuned):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     85307\n",
      "           1       0.60      0.52      0.56       136\n",
      "\n",
      "    accuracy                           1.00     85443\n",
      "   macro avg       0.80      0.76      0.78     85443\n",
      "weighted avg       1.00      1.00      1.00     85443\n",
      "\n",
      "Confusion Matrix (SVM with Sigmoid Kernel - Tuned):\n",
      "[[85260    47]\n",
      " [   65    71]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Create an SVM model with Sigmoid kernel\n",
    "model_sigmoid = SVC(kernel='sigmoid', random_state=42)\n",
    "\n",
    "# Define hyperparameters to tune\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'gamma': ['scale', 'auto', 0.1, 1],\n",
    "    'coef0': [0.1, 0.5, 1]\n",
    "}\n",
    "\n",
    "# Create GridSearchCV object for hyperparameter tuning\n",
    "grid_search = GridSearchCV(model_sigmoid, param_grid, cv=5, n_jobs=-1, verbose=1)\n",
    "\n",
    "# Train the model using the training data with hyperparameter tuning\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters found during the tuning process\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Create an SVM model with the best hyperparameters\n",
    "model_sigmoid_best = SVC(kernel='sigmoid', C=best_params['C'], gamma=best_params['gamma'],\n",
    "                         coef0=best_params['coef0'], random_state=42)\n",
    "\n",
    "# Train the model using the training data with the best hyperparameters\n",
    "model_sigmoid_best.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data for the sigmoid kernel\n",
    "y_pred_sigmoid_best = model_sigmoid_best.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance for the sigmoid kernel\n",
    "accuracy_sigmoid_best = accuracy_score(y_test, y_pred_sigmoid_best)\n",
    "print(\"Accuracy (SVM with Sigmoid Kernel - Tuned):\", accuracy_sigmoid_best)\n",
    "\n",
    "# Print classification report and confusion matrix for the sigmoid kernel\n",
    "print(\"Classification Report (SVM with Sigmoid Kernel - Tuned):\")\n",
    "print(classification_report(y_test, y_pred_sigmoid_best))\n",
    "\n",
    "print(\"Confusion Matrix (SVM with Sigmoid Kernel - Tuned):\")\n",
    "print(confusion_matrix(y_test, y_pred_sigmoid_best))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part B:\n",
    "\n",
    "Problem Statement: Use the following insurance dataset and build a predictive system to predict insurance costs. Demonstrate the steps of data preprocessing and analysis, consider applying train (0.7) and test (0.3), using linear regression to build the model, and evaluate the accuracy of predicting the insurance cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Library Imports:We import the required libraries, including train_test_split from sklearn.model_selection to split the dataset into training and test sets, LinearRegression from sklearn.linear_model to build the linear regression model, mean_squared_error from sklearn.linear_model to build the model, and r2_score from sklearn.metrics to evaluate the model.\n",
    "\n",
    "Install the Data:Using the pandas read_csv function, we load the insurance dataset and save it in the 'data' variable.\n",
    "\n",
    "Data preparation:To get the data ready for modelling, we preprocess the data.\n",
    "A single-hot encoding To turn categorical data (such as gender, smoking status, and location) into numerical values (dummy variables), we utilise pd.get_dummies.We isolate the goal variable (y), which is the 'charges' column, from the feature variables (X).\n",
    "\n",
    "Test-Train Split:With the help of the train_test_split function from sklearn.model_selection, we divided the data into training and test sets.\n",
    "30% of the data are in the test set, while 70% are in the training set.\n",
    "42 is chosen as the random_state to ensure reproducibility.\n",
    "\n",
    "Create the linear regression model and fit it:To the variable \"model,\" we create a LinearRegression object and assign it.\n",
    "We then used the training data (X_train, y_train) to fit the model using the fit technique.\n",
    "During the fitting phase, the model discovers the relationship between the features and the target variable.\n",
    "\n",
    "Construct predictions:Using the predict method, we use the training model to make predictions about the test data (X_test).\n",
    "Model assessment:Mean Squared Error (MSE) and R-squared (R2) score are two measures we use to assess how accurate the model's predictions are.\n",
    "The average squared difference between the anticipated values and the actual values is what MSE calculates. Better performance is indicated by a lower MSE.\n",
    "The R2 score gauges the percentage of the target variable's volatility that can be predicted from the characteristics. A higher R2 value denotes better performance and spans from 0 to 1..\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 33780509.57479163\n",
      "R-squared (R2) Score: 0.7696118054369012\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load the insurance dataset\n",
    "data = pd.read_csv(r\"C:\\Users\\asus\\Downloads\\insurance.csv\")\n",
    "\n",
    "# Convert categorical variables into dummy variables\n",
    "data = pd.get_dummies(data, drop_first=True)\n",
    "\n",
    "# Separate features (X) and target variable (y)\n",
    "X = data.drop('charges', axis=1)\n",
    "y = data['charges']\n",
    "\n",
    "# Split the data into training and test sets (70% train, 30% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create and fit the linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions using the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the Mean Squared Error (MSE) and R-squared (R2) score\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "print(\"R-squared (R2) Score:\", r2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
